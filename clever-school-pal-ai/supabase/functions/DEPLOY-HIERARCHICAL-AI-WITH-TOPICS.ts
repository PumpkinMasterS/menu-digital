import { serve } from "https://deno.land/std@0.168.0/http/server.ts"
import { createClient } from 'https://esm.sh/@supabase/supabase-js@2'

const corsHeaders = {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type, x-api-key',
}

interface StudentContext {
  id: string;
  name: string;
  phone: string;
  school: {
    id: string;
    name: string;
  };
  class: {
    id: string;
    name: string;
    grade: string;
  };
}

interface TopicContent {
  id: string;
  title: string;
  subtitle: string;
  description: string;
  content_data: string;
  learning_objectives: string;
  subject: {
    name: string;
    grade: string;
  };
  difficulty: string;
  similarity: number;
}

serve(async (req) => {
  if (req.method === 'OPTIONS') {
    return new Response('ok', { headers: corsHeaders })
  }

  try {
    const supabaseClient = createClient(
      Deno.env.get('SUPABASE_URL') ?? '',
      Deno.env.get('SUPABASE_SERVICE_ROLE_KEY') ?? ''
    )

    const { messages, phone } = await req.json()
    const userMessage = messages[messages.length - 1]?.content || ""

    console.log("üìû Recebida mensagem de:", phone)
    console.log("üí¨ Mensagem:", userMessage)

    // 1. BUSCAR CONTEXTO DO ESTUDANTE
    const { data: student, error: studentError } = await supabaseClient
      .from('students')
      .select(`
        id, name, phone_number,
        schools!students_school_id_fkey(id, name),
        classes!students_class_id_fkey(id, name, grade)
      `)
      .eq('phone_number', phone)
      .eq('bot_active', true)
      .single()

    if (studentError || !student) {
      console.log("‚ùå Estudante n√£o encontrado ou bot inativo")
      return new Response(
        JSON.stringify({
          reply: "Ol√°! N√£o consegui identificar-te no sistema. Por favor contacta a escola para ativares o bot educativo. üìö"
        }),
        { 
          headers: { ...corsHeaders, 'Content-Type': 'application/json' },
          status: 200 
        }
      )
    }

    const studentContext: StudentContext = {
      id: student.id,
      name: student.name,
      phone: student.phone_number,
      school: {
        id: student.schools.id,
        name: student.schools.name
      },
      class: {
        id: student.classes.id,
        name: student.classes.name,
        grade: student.classes.grade
      }
    }

    console.log("‚úÖ Contexto do estudante:", JSON.stringify(studentContext, null, 2))

    // 2. GERAR EMBEDDING DA PERGUNTA (OpenRouter)
    const openRouterApiKey = Deno.env.get('OPENROUTER_API_KEY')
    const openRouterBaseUrl = Deno.env.get('OPENROUTER_BASE_URL') || 'https://openrouter.ai/api/v1'
    const embeddingModel = Deno.env.get('EMBEDDING_MODEL') || 'mistral/mistral-embed'

    const embeddingResponse = await fetch(`${openRouterBaseUrl}/embeddings`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${openRouterApiKey}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        input: userMessage,
        model: embeddingModel
      })
    })

    if (!embeddingResponse.ok) {
      throw new Error('Erro ao gerar embedding via OpenRouter')
    }

    const embeddingData = await embeddingResponse.json()
    const queryEmbedding = embeddingData.data[0].embedding

    // 3. BUSCAR T√ìPICOS RELEVANTES (BUSCA SEM√ÇNTICA + FILTRO POR TURMA)
    const { data: topics, error: topicsError } = await supabaseClient.rpc(
      'search_topic_content_by_similarity',
      {
        query_embedding: queryEmbedding,
        class_id: studentContext.class.id,
        similarity_threshold: 0.6,
        match_count: 3
      }
    )

    if (topicsError) {
      console.error("‚ùå Erro na busca de t√≥picos:", topicsError)
    }

    console.log("üîç T√≥picos encontrados:", topics?.length || 0)

    // 4. CONSTRUIR CONTEXTO PARA O LLM
    let aiContext = ""
    let availableTopics: TopicContent[] = []

    if (topics && topics.length > 0) {
      availableTopics = topics
      aiContext = topics.map((topic: TopicContent, index: number) => 
        `T√ìPICO ${index + 1}: ${topic.title}
Subt√≠tulo: ${topic.subtitle}
Descri√ß√£o: ${topic.description}
Orienta√ß√µes para ensino: ${topic.content_data}
Objetivos de aprendizagem: ${topic.learning_objectives}
Disciplina: ${topic.subject.name}
Dificuldade: ${topic.difficulty}
Relev√¢ncia: ${(topic.similarity * 100).toFixed(1)}%
---`
      ).join('\n\n')
    }

    // 5. BUSCAR HIST√ìRICO DE CONVERSAS RECENTES
    const { data: recentChats } = await supabaseClient
      .from('chat_logs')
      .select('question, answer')
      .eq('student_id', studentContext.id)
      .order('created_at', { ascending: false })
      .limit(2)

    const chatHistory = recentChats?.map(chat => 
      `Estudante: ${chat.question}\nTutor: ${chat.answer}`
    ).join('\n\n') || ""

    // 6. CONSTRUIR PROMPT PARA O LLM
    const systemPrompt = `√âs um tutor especializado da ${studentContext.school.name} para o estudante ${studentContext.name} da turma ${studentContext.class.name} (${studentContext.class.grade}).

DIRETRIZES R√çGIDAS:
- Responder APENAS sobre t√≥picos curriculares da turma ${studentContext.class.grade}
- M√°ximo 100 palavras por resposta
- Tom did√°tico e apropriado para ${studentContext.class.grade}
- Terminar sempre com uma pergunta espec√≠fica
- Se a pergunta n√£o relaciona com os t√≥picos dispon√≠veis, listar os t√≥picos que podes ensinar

T√ìPICOS DISPON√çVEIS PARA ESTA TURMA:
${aiContext || "Nenhum t√≥pico espec√≠fico encontrado para esta pergunta."}

HIST√ìRICO RECENTE:
${chatHistory}

Se a pergunta do estudante N√ÉO relaciona com nenhum t√≥pico dispon√≠vel, responde:
"Posso ajudar-te com estes t√≥picos da turma ${studentContext.class.name}:
[lista dos t√≥picos principais da turma]

Sobre qual gostarias de aprender?"

Se a pergunta RELACIONA com algum t√≥pico, desenvolve uma explica√ß√£o did√°tica baseada nas orienta√ß√µes do t√≥pico, adaptando a linguagem para ${studentContext.class.grade}.`

    // 7. CHAMAR O LLM (OpenRouter)
    const aiModel = Deno.env.get('AI_MODEL') || 'deepseek/deepseek-chat'
    const llmResponse = await fetch(`${openRouterBaseUrl}/chat/completions`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${openRouterApiKey}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: aiModel,
        messages: [
          { role: "system", content: systemPrompt },
          { role: "user", content: userMessage }
        ],
        temperature: 0.7,
        max_tokens: 150
      })
    })

    if (!llmResponse.ok) {
      throw new Error('Erro ao chamar LLM via OpenRouter')
    }

    const llmData = await llmResponse.json()
    const aiReply = llmData.choices[0].message.content

    console.log("ü§ñ Resposta do AI:", aiReply)

    // 8. GUARDAR NO HIST√ìRICO DE CONVERSAS
    const topicIds = availableTopics.map(topic => topic.id)
    
    await supabaseClient.from('chat_logs').insert({
      student_id: studentContext.id,
      question: userMessage,
      answer: aiReply,
      content_ids: topicIds
    })

    // 9. RESPONDER
    return new Response(
      JSON.stringify({
        reply: aiReply,
        context: {
          student: studentContext.name,
          school: studentContext.school.name,
          class: studentContext.class.name,
          topics_found: availableTopics.length
        }
      }),
      { 
        headers: { ...corsHeaders, 'Content-Type': 'application/json' },
        status: 200 
      }
    )

  } catch (error) {
    console.error('‚ùå Erro geral:', error)
    return new Response(
      JSON.stringify({
        reply: "Desculpa, ocorreu um erro t√©cnico. Tenta novamente em alguns instantes. üîß"
      }),
      { 
        headers: { ...corsHeaders, 'Content-Type': 'application/json' },
        status: 200 
      }
    )
  }
})